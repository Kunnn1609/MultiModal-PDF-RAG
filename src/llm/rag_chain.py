import dashscope
import os
import contextlib
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS

# --- 0. è¾…åŠ©å·¥å…·ï¼šä¸­æ–‡è·¯å¾„è¡¥ä¸ ---
@contextlib.contextmanager
def temporary_chdir(path):
    old_cwd = os.getcwd()
    os.chdir(path)
    try: yield
    finally: os.chdir(old_cwd)

# --- 1. Rerank (é‡æ’åº) ---
try:
    from src.rag.reranker import get_reranker
    def rerank_documents(query, docs, top_k=3):
        if not docs: return []
        reranker = get_reranker()
        pairs = [[query, d.page_content] for d in docs]
        scores = reranker.compute_score(pairs)
        if isinstance(scores, float): scores = [scores]
        doc_score_pairs = list(zip(docs, scores))
        doc_score_pairs.sort(key=lambda x: x[1], reverse=True)
        return [doc for doc, score in doc_score_pairs[:top_k]]
except ImportError:
    def rerank_documents(query, docs, top_k=3):
        return docs[:top_k]

# --- 2. API é…ç½® ---
load_dotenv()
dashscope.api_key = os.getenv("DASHSCOPE_API_KEY")

# --- 3. æŸ¥è¯¢æ”¹å†™ ---
def rewrite_query(user_query, chat_history):
    if not chat_history: return user_query
    recent = chat_history[-2:]
    history_text = "\n".join([f"{'ç”¨æˆ·' if m['role']=='user' else 'åŠ©æ‰‹'}: {m['content']}" for m in recent])
    prompt = f"ä»»åŠ¡ï¼šæ”¹å†™æé—®ï¼Œè¡¥å…¨æŒ‡ä»£è¯ã€‚\nå†å²ï¼š{history_text}\næé—®ï¼š{user_query}\nç»“æœï¼š"
    try:
        res = dashscope.Generation.call(model='qwen-turbo', messages=[{'role':'user','content':prompt}], result_format='message')
        if res.status_code == 200: return res.output.choices[0].message.content.strip()
    except: pass
    return user_query

# --- 4. æ ¸å¿ƒä¸»æµç¨‹ ---
def get_answer_stream(query, db_path, chat_history=[], embedding_model=None):
    if embedding_model is None: raise ValueError("éœ€è¦ embedding_model")
    if not os.path.exists(db_path): raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç´¢å¼•: {db_path}")

    # Step 1: æ”¹å†™
    search_query = rewrite_query(query, chat_history)
    
    # Step 2: åŠ è½½ FAISS
    try:
        with temporary_chdir(db_path):
            vectorstore = FAISS.load_local(".", embedding_model, allow_dangerous_deserialization=True)
    except:
        vectorstore = FAISS.load_local(db_path, embedding_model, allow_dangerous_deserialization=True)

    # Step 3: æ£€ç´¢ (æµ·é‡å¬å›)
    # å¬å› 20 æ¡ï¼Œç¡®ä¿è¦†ç›–å…¨æ–‡ä¸»è¦å†…å®¹
    retrieved_docs = vectorstore.similarity_search(search_query, k=20)

    # Step 4: Rerank (ç²¾é€‰)
    # é€‰å‡ºæœ€ç›¸å…³çš„ 10 æ¡ç»™å¤§æ¨¡å‹
    final_docs = rerank_documents(search_query, retrieved_docs, top_k=10)
    
    # Step 5: æ„å»ºä¸Šä¸‹æ–‡ (ğŸ”¥ å…³é”®ï¼šæ¸…æ´—é¡µç ï¼Œè®© Prompt çœ‹å¾—æ‡‚)
    # æˆ‘ä»¬å…ˆå¯¹æ–‡æ¡£è¿›è¡Œæ’åºï¼Œè®©é¡µç ä»å°åˆ°å¤§ï¼Œç¬¦åˆé˜…è¯»é€»è¾‘
    for doc in final_docs:
        raw_page = doc.metadata.get('source_page') or doc.metadata.get('page_number') or 1
        try:
            val = int(raw_page)
            # å¦‚æœç´¢å¼•æ˜¯0ï¼Œå˜æˆ1ï¼›å¦‚æœæ˜¯1ï¼Œä¿æŒ1ã€‚
            # è¿™é‡Œçš„é€»è¾‘å–å†³äºä½ çš„ Parser å­˜çš„æ˜¯ 0-based è¿˜æ˜¯ 1-basedã€‚
            # å‡è®¾ä¹‹å‰å‡ºç°è¿‡åå·®ï¼Œè¿™é‡Œæˆ‘ä»¬ç»Ÿä¸€ç¡®ä¿æœ€å°æ˜¯ 1ã€‚
            doc.metadata['human_page_number'] = val if val > 0 else 1
        except:
            doc.metadata['human_page_number'] = 1

    # æŒ‰é¡µç æ’åº
    final_docs.sort(key=lambda x: x.metadata['human_page_number'])

    context_list = []
    for doc in final_docs:
        p = doc.metadata['human_page_number']
        # è¿™é‡Œçš„æ ¼å¼è¦éå¸¸æ¸…æ™°ï¼Œè®© AI çŸ¥é“è¿™ä¸€æ®µè¯å±äºå“ªä¸€é¡µ
        context_list.append(f"ã€ç¬¬ {p} é¡µå†…å®¹ã€‘:\n{doc.page_content}")
    
    context_str = "\n\n".join(context_list) if context_list else "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚"

    # Step 6: Prompt (ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šå¼ºåˆ¶å¼•ç”¨æ ¼å¼)
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæå…¶ä¸¥è°¨çš„æ–‡æ¡£åˆ†æå‘˜ã€‚ä½ å¿…é¡»å®Œå…¨åŸºäºä¸‹æ–¹çš„ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

### âš ï¸ æ ¸å¿ƒåŸåˆ™ (å¿…é¡»éµå®ˆ)ï¼š
1.  **é›¶å¤–éƒ¨çŸ¥è¯†**ï¼šä½ çš„å¤§è„‘é‡Œåªæœ‰ä¸‹æ–¹çš„ã€å‚è€ƒèµ„æ–™ã€‘ï¼Œå¿˜æ‰ä½ è®­ç»ƒè¿‡çš„å…¶ä»–çŸ¥è¯†ã€‚å¦‚æœèµ„æ–™é‡Œæ²¡æåˆ°çš„å†…å®¹ï¼Œç›´æ¥è¯´â€œèµ„æ–™æœªæåŠâ€ã€‚
2.  **å¼ºåˆ¶å¼•ç”¨æ ¼å¼**ï¼šä½ çš„å›ç­”ä¸­ï¼Œ**æ¯ä¸€å¥**äº‹å®é™ˆè¿°ã€æ•°æ®å¼•ç”¨æˆ–è§‚ç‚¹æ€»ç»“ï¼Œéƒ½å¿…é¡»åœ¨è¯¥å¥ç»“å°¾åŠ ä¸Šæ¥æºï¼Œæ ¼å¼ä¸¥æ ¼ä¸ºï¼š`(æ¥è‡ªç¬¬xé¡µ)`ã€‚
    * âŒ é”™è¯¯ï¼šæ ¹æ®æ–‡æ¡£ï¼Œè¥æ”¶å¢é•¿äº†ã€‚
    * âœ… æ­£ç¡®ï¼š2023å¹´è¥æ”¶å¢é•¿äº†20%(æ¥è‡ªç¬¬5é¡µ)ã€‚
    * âœ… æ­£ç¡®ï¼šä½œè€…è®¤ä¸ºæ—¶é—´ç®¡ç†æ˜¯éª—å±€(æ¥è‡ªç¬¬1é¡µ)ï¼ŒçœŸæ­£çš„å…³é”®æ˜¯æ³¨æ„åŠ›ç®¡ç†(æ¥è‡ªç¬¬2é¡µ)ã€‚
3.  **é¡µç å¯¹åº”**ï¼šå‚è€ƒèµ„æ–™ä¸­æ ‡è®°ä¸ºã€ç¬¬ x é¡µå†…å®¹ã€‘ï¼Œä½ çš„å¼•ç”¨å°±å¿…é¡»å†™ `(æ¥è‡ªç¬¬xé¡µ)`ï¼Œä¸è¦è‡ªå·±åŠ å‡æ•°å­—ã€‚

### å‚è€ƒèµ„æ–™ï¼š
{context_str}
"""

    messages = [
        {'role': 'system', 'content': system_prompt},
        {'role': 'user', 'content': query}
    ]

    responses = dashscope.Generation.call(
        model='qwen-turbo', # æˆ–è€… qwen-plus æ•ˆæœæ›´å¥½
        messages=messages,
        result_format='message',
        stream=True,
        incremental_output=True
    )
    
    return responses, final_docs