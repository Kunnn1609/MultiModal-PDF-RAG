import dashscope
import os
import contextlib
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS

# --- 0. è¾…åŠ©å·¥å…· ---
@contextlib.contextmanager
def temporary_chdir(path):
    old_cwd = os.getcwd()
    os.chdir(path)
    try: yield
    finally: os.chdir(old_cwd)

# --- 1. Rerank ---
try:
    from src.rag.reranker import get_reranker
    def rerank_documents(query, docs, top_k=3):
        if not docs: return []
        reranker = get_reranker()
        pairs = [[query, d.page_content] for d in docs]
        scores = reranker.compute_score(pairs)
        if isinstance(scores, float): scores = [scores]
        doc_score_pairs = list(zip(docs, scores))
        doc_score_pairs.sort(key=lambda x: x[1], reverse=True)
        return [doc for doc, score in doc_score_pairs[:top_k]]
except ImportError:
    def rerank_documents(query, docs, top_k=3):
        return docs[:top_k]

# --- 2. API é…ç½® ---
load_dotenv()
dashscope.api_key = os.getenv("DASHSCOPE_API_KEY")

# --- 3. æŸ¥è¯¢æ”¹å†™ ---
def rewrite_query(user_query, chat_history):
    if not chat_history: return user_query
    recent = chat_history[-2:]
    history_text = "\n".join([f"{'ç”¨æˆ·' if m['role']=='user' else 'åŠ©æ‰‹'}: {m['content']}" for m in recent])
    prompt = f"ä»»åŠ¡ï¼šæ”¹å†™æé—®ï¼Œè¡¥å…¨æŒ‡ä»£è¯ã€‚\nå†å²ï¼š{history_text}\næé—®ï¼š{user_query}\nç»“æœï¼š"
    try:
        # æ”¹å†™ä¸éœ€è¦å¤ªä¸¥è°¨ï¼Œtemperature ä¿æŒé»˜è®¤å³å¯
        res = dashscope.Generation.call(model='qwen-turbo', messages=[{'role':'user','content':prompt}], result_format='message')
        if res.status_code == 200: return res.output.choices[0].message.content.strip()
    except: pass
    return user_query

# --- 4. æ ¸å¿ƒä¸»æµç¨‹ ---
def get_answer_stream(query, db_path, chat_history=[], embedding_model=None):
    if embedding_model is None: raise ValueError("éœ€è¦ embedding_model")
    if not os.path.exists(db_path): raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç´¢å¼•: {db_path}")

    # Step 1: æ”¹å†™
    search_query = rewrite_query(query, chat_history)
    
    # Step 2: åŠ è½½ FAISS
    try:
        with temporary_chdir(db_path):
            vectorstore = FAISS.load_local(".", embedding_model, allow_dangerous_deserialization=True)
    except:
        vectorstore = FAISS.load_local(db_path, embedding_model, allow_dangerous_deserialization=True)

    # Step 3: æ£€ç´¢
    retrieved_docs = vectorstore.similarity_search(search_query, k=20)

    # Step 4: Rerank
    final_docs = rerank_documents(search_query, retrieved_docs, top_k=10)
    
    # Step 5: æ„å»ºä¸Šä¸‹æ–‡
    for doc in final_docs:
        raw_page = doc.metadata.get('source_page') or doc.metadata.get('page_number') or 1
        try:
            val = int(raw_page)
            doc.metadata['human_page_number'] = val if val > 0 else 1
        except:
            doc.metadata['human_page_number'] = 1

    final_docs.sort(key=lambda x: x.metadata['human_page_number'])

    context_list = []
    for doc in final_docs:
        p = doc.metadata['human_page_number']
        context_list.append(f"ã€ç¬¬ {p} é¡µå†…å®¹ã€‘:\n{doc.page_content}")
    
    context_str = "\n\n".join(context_list) if context_list else "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚"

    # Step 6: Prompt (ğŸ”¥ ä¼˜åŒ–é‡ç‚¹ï¼šç»“æ„åŒ–æ€ç»´é“¾ Prompt)
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ·±åº¦é˜…è¯»åŠ©æ‰‹ã€‚è¯·åŸºäºã€å‚è€ƒèµ„æ–™ã€‘å›ç­”é—®é¢˜ã€‚

### âš ï¸ å›ç­”è§„èŒƒ (å¿…é¡»ä¸¥æ ¼éµå®ˆ)ï¼š
1.  **ç»“æ„åŒ–è¾“å‡º**ï¼šè¯·åŠ¡å¿…ä½¿ç”¨ **Markdown åˆ—è¡¨ (Bullet Points)** çš„å½¢å¼æ¥ç»„ç»‡ç­”æ¡ˆï¼Œä¸è¦å†™æˆä¸€å¤§æ®µé•¿æ–‡ã€‚
2.  **ç»†èŠ‚ä¼˜å…ˆ**ï¼šä¸è¦åªå†™å®è§‚æ¦‚å¿µï¼Œè¦æå–å…·ä½“çš„**æ–¹æ³•è®ºã€æ­¥éª¤ã€æ ¸å¿ƒè§‚ç‚¹**ã€‚
    * âŒ å·®çš„å›ç­”ï¼šæ–‡ç« ä»‹ç»äº†æ—¶é—´ç®¡ç†çš„æ–¹æ³•ã€‚
    * âœ… å¥½çš„å›ç­”ï¼šæ–‡ç« æå‡ºäº† "5 AM Project"ï¼Œå»ºè®®åˆ©ç”¨æ—©ä¸Š5ç‚¹çš„æ—¶é—´åšæœ€é‡è¦çš„äº‹ (æ¥è‡ªç¬¬2é¡µ)ã€‚
3.  **å¼ºåˆ¶å¼•ç”¨**ï¼šæ¯ä¸€æ¡è¦ç‚¹åå¿…é¡»æ ‡æ³¨ `(æ¥è‡ªç¬¬xé¡µ)`ã€‚
4.  **é›¶å¤–éƒ¨çŸ¥è¯†**ï¼šä¸¥ç¦ç¼–é€ èµ„æ–™ä¸­æ²¡æœ‰çš„å†…å®¹ã€‚

### å‚è€ƒèµ„æ–™ï¼š
{context_str}
"""

    messages = [
        {'role': 'system', 'content': system_prompt},
        {'role': 'user', 'content': query}
    ]

    # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šæ·»åŠ  temperature å‚æ•°
    responses = dashscope.Generation.call(
        model='qwen-turbo',
        messages=messages,
        result_format='message',
        stream=True,
        incremental_output=True,
        temperature=0.01,  # ğŸ‘ˆ å…³é”®ï¼è®¾ä¸ºæä½å€¼ï¼Œæ¥è¿‘ 0
        top_p=0.8          # è¾…åŠ©å‚æ•°ï¼Œé™åˆ¶è¿‡åº¦å‘æ•£
    )
    
    return responses, final_docs