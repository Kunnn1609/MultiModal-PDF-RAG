from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.retrievers import BM25Retriever

import os
import pickle

# 1. åˆå§‹åŒ–ä¸­æ–‡ Embedding æ¨¡å‹ (æ¨èä½¿ç”¨ BGE æˆ– m3e)
# è¯¥æ¨¡å‹å°†æ–‡å­—è½¬ä¸ºå‘é‡æ•°å­—ï¼Œæ˜¯æ£€ç´¢çš„åŸºç¡€
# db_path = r"D:\workspace\finale_workspace\PDF_RAG_Project\data\chroma_db"

def build_vector_db(full_content, file_name, embedding_model, base_db_path=r"D:\workspace\finale_workspace\PDF_RAG_Project\data\vector_dbs"):
    """
    ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºç‹¬ç«‹çš„å‘é‡åº“æ–‡ä»¶å¤¹
    """
    # 1. ç§»é™¤éæ³•å­—ç¬¦
    safe_name = file_name.replace(".pdf", "").replace(" ", "_")
    # 2. ç¡®å®šæœ€ç»ˆå­˜æ”¾è·¯å¾„
    save_path = os.path.join(base_db_path, safe_name)
    
    # å¦‚æœè¯¥æ–‡ä»¶çš„åº“å·²ç»å­˜åœ¨ï¼Œå¯ä»¥é€‰æ‹©è·³è¿‡æˆ–é‡æ–°è¦†ç›–
    if os.path.exists(save_path):
        print(f"â„¹ï¸ æ–‡ä»¶ {file_name} çš„çŸ¥è¯†åº“å·²å­˜åœ¨ï¼Œå°†ç›´æ¥å¤ç”¨ã€‚")
        # å¦‚æœä½ æƒ³å¼ºåˆ¶è¦†ç›–ï¼Œå¯ä»¥åœ¨è¿™é‡Œç”¨ shutil.rmtree(save_path)
        return save_path

    """
    å°†è§£æå‡ºçš„å†…å®¹åˆ‡ç‰‡å¹¶å­˜å…¥å‘é‡åº“
    full_content: ä¹‹å‰ smart_extract è¿”å›çš„é¡µé¢å­—å…¸åˆ—è¡¨
    """
    
    # 2. é…ç½®åˆ‡ç‰‡å™¨ï¼šæ§åˆ¶å—å¤§å°åœ¨ 500 å­—å·¦å³ï¼Œé‡å  50 å­—
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        length_function=len
    )

    documents = []
    metadatas = []

    # 3. éå†æ¯ä¸€é¡µï¼Œç”Ÿæˆå¸¦é¡µç å…ƒæ•°æ®çš„åˆ‡ç‰‡
    for page in full_content:
        page_text = page['content']
        page_num = page['page_number']
        chunks = text_splitter.split_text(page_text)
        
        for chunk in chunks:
            documents.append(chunk)
            # è¿™é‡Œçš„ metadata æ˜¯åç»­è§£å†³å¹»è§‰ã€å®šä½é¡µç çš„å”¯ä¸€ä¾æ®
            metadatas.append({
                "source_page": page_num,
                "extraction_method": page['method']
            })

    # 4. åˆ›å»ºå¹¶æŒä¹…åŒ–å‘é‡åº“
    print(f"ğŸ“¦ æ­£åœ¨æ„å»ºå‘é‡åº“ï¼Œå½“å‰å…±æœ‰ {len(documents)} ä¸ªçŸ¥è¯†åˆ‡ç‰‡...")
    vectordb = Chroma.from_texts(
        texts=documents,
        embedding=embedding_model,
        metadatas=metadatas,
        persist_directory=save_path
    )
    
    # 5. ã€æ ¸å¿ƒä¼˜åŒ–ã€‘æ„å»ºå¹¶ä¿å­˜ BM25 æ£€ç´¢å™¨æ‰€éœ€çš„æ•°æ®
    # BM25 ä¸åƒ Chroma èƒ½è‡ªåŠ¨æŒä¹…åŒ–ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨ä¿å­˜æ–‡æ¡£åˆ—è¡¨
    print(f"ğŸ§¬ æ­£åœ¨ç”Ÿæˆå…³é”®è¯ç´¢å¼• (BM25)...")
    bm25_data = {
        "documents": documents,
        "metadatas": metadatas
    }
    with open(os.path.join(save_path, "bm25_data.pkl"), "wb") as f:
        pickle.dump(bm25_data, f)

    print(f"âœ… æ··åˆç´¢å¼•æ„å»ºæˆåŠŸï¼è·¯å¾„: {save_path}")
    return vectordb

if __name__ == "__main__":
    # --- å…¨é“¾è·¯æµ‹è¯• (å› ä¸ºå‡½æ•°æ”¹äº†ï¼Œè¿™é‡Œæµ‹è¯•ä»£ç ä¹Ÿè¦æ”¹) ---
    import sys
    from langchain_community.embeddings import HuggingFaceEmbeddings # ä»…æµ‹è¯•æ—¶å¯¼å…¥
    
    # æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
    src_dir = os.path.join(os.path.dirname(__file__), '..')
    src_dir = os.path.abspath(src_dir)
    if src_dir not in sys.path:
        sys.path.insert(0, src_dir)
    
    from parser.smart_parser import smart_extract
    from paddleocr import PaddleOCR
    
    # 1. åˆå§‹åŒ–æ¨¡å‹ (è¿™æ˜¯æ¨¡æ‹Ÿ app.py çš„è¡Œä¸º)
    print("â³ æµ‹è¯•æ¨¡å¼ï¼šæ­£åœ¨åˆå§‹åŒ– Embedding æ¨¡å‹...")
    test_embedding_model = HuggingFaceEmbeddings(model_name="shibing624/text2vec-base-chinese")
    
    # 2. æ¨¡æ‹Ÿè§£æ
    engine = PaddleOCR(lang="ch", use_angle_cls=True)
    # æ›¿æ¢ä¸ºä½ æœ¬åœ°çœŸå®å­˜åœ¨çš„ PDF è·¯å¾„
    test_pdf = r"D:\workspace\finale_workspace\PDF_RAG_Project\data\raw\test.pdf" 
    
    if os.path.exists(test_pdf):
        print("ğŸ” å¼€å§‹è§£æ PDF...")
        pages_data = smart_extract(test_pdf, engine)
        
        # 3. å­˜å…¥æ•°æ®åº“ (ä¼ å…¥æ¨¡å‹)
        print("ğŸ’¾ å¼€å§‹å»ºåº“...")
        # æ³¨æ„ï¼šè¿™é‡Œä¼ å…¥äº† test_embedding_model
        db = build_vector_db(pages_data, "test.pdf", test_embedding_model)
        
        if db:
            # 4. éªŒè¯æ£€ç´¢åŠŸèƒ½
            query = "æµ‹è¯•æé—®"
            print(f"\nğŸ” æµ‹è¯•æ£€ç´¢æé—®: {query}")
            results = db.similarity_search(query, k=2)
            for doc in results:
                print(f"[P{doc.metadata['source_page']}] {doc.page_content[:50]}...")
    else:
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_pdf}")