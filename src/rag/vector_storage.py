import os
import shutil
import time
import contextlib
from langchain.schema import Document 
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

@contextlib.contextmanager
def temporary_chdir(path):
    """
    ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼šä¸´æ—¶åˆ‡æ¢å·¥ä½œç›®å½•
    ç”¨äºè§£å†³ FAISS C++ å±‚æ— æ³•å¤„ç†ä¸­æ–‡ç»å¯¹è·¯å¾„çš„ Bug
    """
    old_cwd = os.getcwd()
    os.chdir(path)
    try:
        yield
    finally:
        os.chdir(old_cwd)

def build_vector_db(docs, db_name, embedding_model):
    """
    ä½¿ç”¨ FAISS æ„å»ºå‘é‡ç´¢å¼• (ä¿®å¤ï¼šæ­£ç¡®è¯»å– smart_parser çš„å…ƒæ•°æ®)
    """
    base_path = r"D:\workspace\finale_workspace\PDF_RAG_Project\data\vector_dbs"
    target_dir = os.path.join(base_path, db_name)
    
    # --- 1. æ•°æ®æ¸…æ´—ä¸å…ƒæ•°æ®æå– ---
    doc_objects = []
    
    for i, d in enumerate(docs):
        content = ""
        meta = {}
        
        if isinstance(d, dict):
            # 1. æå–å†…å®¹
            content = d.get("page_content") or d.get("text") or d.get("content") or ""
            
            # 2. æå–å…ƒæ•°æ® (æ ¸å¿ƒä¿®å¤)
            # smart_parser è¿”å›çš„æ˜¯æ‰å¹³å­—å…¸ï¼Œæˆ‘ä»¬éœ€è¦æŠŠé content çš„å­—æ®µéƒ½æ”¾å…¥ meta
            # ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦å­˜åœ¨æ˜¾å¼çš„ 'page_number' (æ¥è‡ª parser)
            if "page_number" in d:
                meta["source_page"] = d["page_number"]
            if "method" in d:
                meta["method"] = d["method"]
                
            # å…¼å®¹å…¶ä»–æ ¼å¼ï¼šå¦‚æœçœŸæœ‰ metadata é”®ï¼Œä¹Ÿåˆå¹¶è¿›æ¥
            if "metadata" in d:
                meta.update(d["metadata"])
                
        else:
            # å…¼å®¹ Document å¯¹è±¡
            content = getattr(d, "page_content", "")
            meta = getattr(d, "metadata", {})

        # 3. å…œåº•é€»è¾‘ï¼šå¦‚æœç»è¿‡ä¸Šè¿°æ­¥éª¤è¿˜æ˜¯æ²¡æœ‰é¡µç ï¼Œä½¿ç”¨ i+1
        if "source_page" not in meta:
            meta["source_page"] = i + 1

        content = str(content)
        if not content or not content.strip():
            continue
            
        doc_objects.append(Document(page_content=content, metadata=meta))

    if not doc_objects:
        print("âš ï¸ [RAG] è­¦å‘Šï¼šæ²¡æœ‰æœ‰æ•ˆæ–‡æ¡£ã€‚")
        return None
    
    # --- 2. æ¸…ç†æ—§æ•°æ® ---
    if os.path.exists(target_dir):
        try:
            shutil.rmtree(target_dir)
            print(f"ğŸ§¹ æ—§ç´¢å¼•å·²æ¸…ç†: {target_dir}")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†æ—§æ–‡ä»¶å¤±è´¥: {e}")

    # --- 3. åˆ‡åˆ†æ–‡æ¡£ ---
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500, 
        chunk_overlap=50
    )
    split_docs = text_splitter.split_documents(doc_objects)
    
    print(f"ğŸ“„ æ–‡æ¡£åˆ‡åˆ†å®Œæˆ: {len(doc_objects)} é¡µ -> {len(split_docs)} ä¸ªåˆ‡ç‰‡")
    
    # [Debug] æ‰“å°æ£€æŸ¥
    if len(split_docs) > 0:
        print(f"ğŸ› [Debug Check] ç¬¬ä¸€å—å…ƒæ•°æ®: {split_docs[0].metadata}")
        if len(split_docs) > 5:
            print(f"ğŸ› [Debug Check] ç¬¬äº”å—å…ƒæ•°æ®: {split_docs[5].metadata}")

    # --- 4. æ„å»ºå¹¶ä¿å­˜ FAISS ç´¢å¼• ---
    try:
        print("ğŸš€ æ­£åœ¨æ„å»º FAISS å†…å­˜ç´¢å¼•...")
        vectorstore = FAISS.from_documents(
            documents=split_docs, 
            embedding=embedding_model
        )
        
        if not os.path.exists(target_dir):
            os.makedirs(target_dir)
        
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ç´¢å¼•åˆ°: {target_dir}")
        with temporary_chdir(target_dir):
            vectorstore.save_local(".")
            
        print(f"âœ… [RAG] FAISS ç´¢å¼•ä¿å­˜æˆåŠŸï¼")
        
    except Exception as e:
        print(f"âŒ [RAG] ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None
        
    return target_dir