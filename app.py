import streamlit as st
import os
import shutil
import time
import fitz  # PyMuPDF
from langchain_community.embeddings import DashScopeEmbeddings
from paddleocr import PaddleOCR
from dotenv import load_dotenv

# å¼•å…¥åç«¯æ¨¡å—
from src.parser.smart_parser import smart_extract
from src.rag.vector_storage import build_vector_db
from src.llm.rag_chain import get_answer_stream

st.set_page_config(page_title="æ™ºèƒ½æ–‡æ¡£ä¸“å®¶ (Ultimate)", page_icon="âš¡", layout="wide")
load_dotenv()

RAW_DATA_DIR = os.path.join("data", "raw")
DB_DATA_DIR = os.path.join("data", "vector_dbs")
os.makedirs(RAW_DATA_DIR, exist_ok=True)
os.makedirs(DB_DATA_DIR, exist_ok=True)

if 'uploader_key' not in st.session_state: st.session_state.uploader_key = 0

@st.cache_resource
def load_ocr_engine():
    return PaddleOCR(use_angle_cls=True, lang="ch")

def render_pdf_page_as_image(pdf_path, human_page_num):
    if not os.path.exists(pdf_path): return None
    try:
        doc = fitz.open(pdf_path)
        try: page_index = int(human_page_num) - 1 
        except: page_index = 0
        if page_index < 0: page_index = 0
        if page_index >= len(doc): page_index = len(doc) - 1
        page = doc.load_page(page_index)
        
        # ğŸ¨ ã€ä¼˜åŒ– 1ã€‘é™ä½æ¸²æŸ“å€ç‡ï¼š1.5å€è¶³å¤Ÿæ¸…æ™°ï¼Œä¸”å›¾ç‰‡æ›´å°æ›´è½»é‡
        pix = page.get_pixmap(matrix=fitz.Matrix(1.5, 1.5)) 
        return pix.tobytes()
    except: return None

def delete_project_completely(clean_filename):
    pdf_path = os.path.join(RAW_DATA_DIR, f"{clean_filename}.pdf")
    db_path = os.path.join(DB_DATA_DIR, clean_filename)
    if 'current_db' in st.session_state and st.session_state['current_db'] == db_path:
        del st.session_state['current_db']
    if 'last_selected' in st.session_state and st.session_state['last_selected'] == f"{clean_filename}.pdf":
        del st.session_state['last_selected']
    if os.path.exists(db_path):
        try: shutil.rmtree(db_path)
        except: return False
    if os.path.exists(pdf_path):
        try: os.remove(pdf_path)
        except: return False
    return True

# --- ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šé’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–çš„è¯„åˆ†ç®—æ³• ---

def calculate_metrics(question, answer, source_docs):
    """
    ä½¿ç”¨å­—ç¬¦çº§ (Character-level) Jaccard ç›¸ä¼¼åº¦æ¥è¯„ä¼°ä¸­æ–‡è´¨é‡
    """
    context_text = "".join([d.page_content for d in source_docs])
    ignore_chars = set(" ï¼Œã€‚ï¼ï¼Ÿã€\n\t*`")
    ans_chars = set(answer) - ignore_chars
    ctx_chars = set(context_text) - ignore_chars
    q_chars = set(question) - ignore_chars
    
    if not ans_chars: 
        return {"faithfulness": 0.0, "relevance": 0.0, "evidence": 0.0}

    overlap_chars = ans_chars.intersection(ctx_chars)
    faithfulness = len(overlap_chars) / len(ans_chars)

    if not q_chars:
        relevance = 0.0
    else:
        q_overlap = ans_chars.intersection(q_chars)
        relevance = min((len(q_overlap) / len(q_chars)) * 2.0, 1.0)

    evidence_score = min(len(source_docs) / 4, 1.0)

    return {
        "faithfulness": faithfulness,
        "relevance": relevance,
        "evidence": evidence_score
    }

def generate_expert_critique(metrics):
    critiques = []
    f = metrics['faithfulness']
    if f > 0.85: critiques.append("âœ… **å¯ä¿¡åº¦æé«˜**ï¼šå›ç­”ä¸¥æ ¼åŸºäºåŸæ–‡ã€‚")
    elif f > 0.6: critiques.append("âš ï¸ **å¯ä¿¡åº¦ä¸€èˆ¬**ï¼šåŒ…å«éƒ¨åˆ†æ€»ç»“æ€§æªè¾ã€‚")
    else: critiques.append("ğŸš« **å­˜åœ¨å¹»è§‰é£é™©**ï¼šå¤§é‡ç”¨è¯æœªåœ¨åŸæ–‡å‡ºç°ï¼Œè¯·æ ¸å¯¹ã€‚")
    
    r = metrics['relevance']
    if r > 0.6: critiques.append("ğŸ¯ **åˆ‡é¢˜ç²¾å‡†**ï¼šç´§æ‰£é—®é¢˜æ ¸å¿ƒã€‚")
    elif r > 0.3: critiques.append("ğŸ‘Œ **åŸºæœ¬åˆ‡é¢˜**ï¼šå›ç­”äº†ä¸»è¦æ–¹é¢ã€‚")
    else: critiques.append("ğŸ¤” **ç­”éæ‰€é—®**ï¼šæœªåŒ…å«é—®é¢˜å…³é”®è¯ã€‚")
    
    e = metrics['evidence']
    if e >= 0.75: critiques.append("ğŸ“š **å¼•ç”¨ä¸°å¯Œ**ï¼šè®ºè¯æ‰å®ã€‚")
    else: critiques.append("ğŸ” **èµ„æ–™è¾ƒå°‘**ï¼šä»…æ£€ç´¢åˆ°å°‘é‡ç‰‡æ®µã€‚")
    
    return "\n\n".join(critiques)

# ================= ä¾§è¾¹æ  =================
with st.sidebar:
    st.header("ğŸ“š FAISS ä¹¦æ¶")
    
    # ğŸ›‘ ã€ä¼˜åŒ– 2ã€‘æ·»åŠ â€œä¸­æ­¢ç”Ÿæˆâ€æŒ‰é’® (æ–°å¢å†…å®¹)
    if st.button("â¹ï¸ ä¸­æ­¢/é‡ç½®", type="primary"):
        st.rerun()
        
    st.divider()
    
    uploaded_file = st.file_uploader("â• ä¸Šä¼ ", type=["pdf"], key=f"uploader_{st.session_state.uploader_key}")
    if uploaded_file:
        file_name = uploaded_file.name
        save_path = os.path.join(RAW_DATA_DIR, file_name)
        if not os.path.exists(save_path):
            with open(save_path, "wb") as f: f.write(uploaded_file.getbuffer())
            st.toast(f"âœ… {file_name} å…¥åº“")
            st.session_state.uploader_key += 1
            time.sleep(0.5)
            st.rerun()
    
    st.divider()
    local_files = [f for f in os.listdir(RAW_DATA_DIR) if f.lower().endswith('.pdf')]
    if local_files:
        idx = 0
        if 'last_selected' in st.session_state and st.session_state['last_selected'] in local_files:
            idx = local_files.index(st.session_state['last_selected'])
        selected_file = st.selectbox("ğŸ“‚ é€‰æ‹©æ–‡æ¡£", local_files, index=idx)
        st.session_state['last_selected'] = selected_file
        
        if selected_file:
            clean_name = os.path.splitext(selected_file)[0].strip()
            pdf_path = os.path.join(RAW_DATA_DIR, selected_file)
            db_path = os.path.join(DB_DATA_DIR, clean_name)
            st.session_state['current_pdf_path'] = pdf_path
            st.session_state['current_db'] = db_path
            
            if os.path.exists(os.path.join(db_path, "index.faiss")):
                st.success("âœ… å·²è§£æ")
                if st.button("ğŸ—‘ï¸ åˆ é™¤"):
                    if delete_project_completely(clean_name):
                        st.rerun()
            else:
                st.warning("âš ï¸ æœªè§£æ")
                if st.button("ğŸš€ è§£æ"):
                    with st.spinner("è§£æä¸­..."):
                        try:
                            ocr = load_ocr_engine()
                            embed = DashScopeEmbeddings(model="text-embedding-v1")
                            raw = smart_extract(pdf_path, ocr)
                            build_vector_db(raw, clean_name, embed)
                            st.success("å®Œæˆ")
                            st.rerun()
                        except Exception as e: st.error(str(e))

# ================= ä¸»ç•Œé¢ =================
st.title("âš¡ PDFæ™ºèƒ½æ–‡æ¡£ä¸“å®¶")

if 'messages' not in st.session_state: st.session_state.messages = []
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]): st.markdown(msg["content"])

if prompt := st.chat_input("æé—®..."):
    current_db = st.session_state.get('current_db')
    current_pdf = st.session_state.get('current_pdf_path')
    
    if not current_db or not os.path.exists(os.path.join(current_db, "index.faiss")):
        st.toast("âŒ è¯·å…ˆè§£ææ–‡æ¡£")
        st.stop()

    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"): st.markdown(prompt)

    with st.chat_message("assistant"):
        placeholder = st.empty()
        full_response = ""
        embed = DashScopeEmbeddings(model="text-embedding-v1")
        
        try:
            response_stream, source_docs = get_answer_stream(prompt, current_db, st.session_state.messages, embed)
            
            for chunk in response_stream:
                if chunk.status_code == 200:
                    content = chunk.output.choices[0].message.content
                    full_response += content
                    placeholder.markdown(full_response + "â–Œ")
            placeholder.markdown(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})

            if source_docs:
                unique_pages = sorted(list(set(
                    [doc.metadata.get('human_page_number', 1) for doc in source_docs]
                )))
                
                st.divider()
                st.markdown(f"**ğŸ“š å¼•ç”¨æ¥æº ({len(unique_pages)} é¡µ)**")
                
                # ğŸ¨ ã€ä¼˜åŒ– 3ã€‘æ”¹ä¸ºåŒåˆ—å¸ƒå±€ (ä¿®æ”¹éƒ¨åˆ†)
                cols = st.columns(2)
                
                for idx, page_num in enumerate(unique_pages):
                    # å°†å›¾ç‰‡åˆ†é…åˆ°å·¦å³ä¸¤åˆ—
                    with cols[idx % 2]:
                        with st.expander(f"ğŸ“„ ç¬¬ {page_num} é¡µåŸæ–‡å¿«ç…§", expanded=True):
                            relevant_text = next((d.page_content for d in source_docs if d.metadata.get('human_page_number') == page_num), "...")
                            st.caption(f"ç›¸å…³å†…å®¹æ‘˜å½•: ...{relevant_text[:100]}...")
                            if current_pdf and os.path.exists(current_pdf):
                                img_bytes = render_pdf_page_as_image(current_pdf, page_num)
                                # use_column_width=True é…åˆ columns(2) ä¼šè‡ªåŠ¨ç¼©å°å›¾ç‰‡
                                if img_bytes: st.image(img_bytes, use_column_width=True)
            
            # è®¡ç®—æŒ‡æ ‡
            scores = calculate_metrics(prompt, full_response, source_docs)
            
            # --- ä»ªè¡¨ç›˜å±•ç¤º ---
            st.divider()
            st.subheader("ğŸ“Š è´¨é‡è¯„ä¼°")
            c1, c2, c3 = st.columns(3)
            c1.metric("ğŸ›¡ï¸ å¿ å®åº¦", f"{scores['faithfulness']*100:.0f}%")
            c1.progress(scores['faithfulness'])
            c2.metric("ğŸ¯ ç›¸å…³æ€§", f"{scores['relevance']*100:.0f}%")
            c2.progress(scores['relevance'])
            c3.metric("ğŸ“š å¼•ç”¨æ•°", len(source_docs))
            c3.progress(scores['evidence'])
            
            st.info(f"**ğŸ§‘â€ğŸ« ä¸“å®¶ç‚¹è¯„ï¼š**\n\n{generate_expert_critique(scores)}")
            
        except Exception as e:
            st.error(f"Error: {e}")
            import traceback
            st.code(traceback.format_exc())